import argparse
import requests
import time
import csv
import os
import random
import re
from xml.etree import ElementTree as ET
from bs4 import BeautifulSoup
from fake_useragent import UserAgent  # éœ€å®‰è£…ï¼špip install fake_useragent

# -------------------------- å…¨å±€é…ç½® --------------------------
DEFAULT_SEARCH_PHRASE = "medical image registration"  # é»˜è®¤æ ¸å¿ƒå…³é”®è¯ï¼šåŒ»å­¦å½±åƒé…å‡†
SEARCH_PHRASE = DEFAULT_SEARCH_PHRASE  # å½“å‰ä½¿ç”¨çš„å…³é”®è¯
MAX_RESULTS_PER_SITE = 5  
BASE_PDF_SAVE_DIR = "./multi_source_pdfs"  # PDFä¿å­˜åŸºç¡€ç›®å½•
PDF_SAVE_DIR = f"{BASE_PDF_SAVE_DIR}/{SEARCH_PHRASE.replace(' ', '_')}"  # PDFä¿å­˜ç›®å½•
BASE_CSV_PATH = "./multi_source_papers"  # ç»“æœè®°å½•CSVåŸºç¡€è·¯å¾„
CSV_PATH = f"{BASE_CSV_PATH}_{SEARCH_PHRASE.replace(' ', '_')}.csv"  # ç»“æœè®°å½•CSV
REQUEST_DELAY = (2, 4)  # éšæœºè¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé™ä½åçˆ¬é£é™©
DOWNLOAD_RETRIES = 3  # ä¸‹è½½å¤±è´¥é‡è¯•æ¬¡æ•°
TIMEOUT = 60  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# åˆ›å»ºä¿å­˜ç›®å½•
os.makedirs(PDF_SAVE_DIR, exist_ok=True)
ua = UserAgent()  # éšæœºUser-Agentç”Ÿæˆå™¨


# -------------------------- å·¥å…·å‡½æ•° --------------------------
def get_random_headers():
    """ç”Ÿæˆéšæœºè¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿä¸åŒæµè§ˆå™¨"""
    return {
        "User-Agent": ua.random,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Connection": "keep-alive",
        "Referer": "https://www.ncbi.nlm.nih.gov/",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Upgrade-Insecure-Requests": "1"
    }


def get_pmc_specific_headers():
    """ç”Ÿæˆé’ˆå¯¹PMC PDFä¸‹è½½çš„ç‰¹å®šè¯·æ±‚å¤´"""
    return {
        "User-Agent": ua.random,
        "Accept": "application/pdf,application/x-pdf,*/*",
        "Accept-Language": "en-US,en;q=0.9",
        "Connection": "keep-alive",
        "Referer": "https://www.ncbi.nlm.nih.gov/pmc/articles/",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Upgrade-Insecure-Requests": "1",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache"
    }


def is_valid_pdf(file_path):
    """éªŒè¯PDFæœ‰æ•ˆæ€§ï¼ˆæ£€æŸ¥æ–‡ä»¶å¤´å’Œå¤§å°ï¼‰"""
    if not os.path.exists(file_path) or os.path.getsize(file_path) < 1024 * 10:  # è‡³å°‘10KB
        return False
    with open(file_path, "rb") as f:
        return f.read(5) == b"%PDF-"  # PDFæ–‡ä»¶å¤´æ ‡è¯†


def safe_download(pdf_url, save_path):
    """å¸¦é‡è¯•æœºåˆ¶çš„PDFä¸‹è½½ï¼Œè§£å†³è¿æ¥é‡ç½®/è¶…æ—¶é—®é¢˜"""
    if is_valid_pdf(save_path):
        print(f"âœ… å·²å­˜åœ¨æœ‰æ•ˆPDFï¼š{os.path.basename(save_path)}")
        return True
    for retry in range(DOWNLOAD_RETRIES):
        try:
            # æ ¹æ®URLé€‰æ‹©åˆé€‚çš„è¯·æ±‚å¤´
            if "pmc" in pdf_url.lower():
                headers = get_pmc_specific_headers()
            else:
                headers = get_random_headers()
            print(f"ğŸ“¥ ä¸‹è½½PDFï¼ˆé‡è¯•{retry+1}/{DOWNLOAD_RETRIES}ï¼‰ï¼š{pdf_url}")
            response = requests.get(
                pdf_url,
                headers=headers,
                timeout=TIMEOUT,
                stream=True,  # æµå¼ä¸‹è½½å¤§æ–‡ä»¶
                allow_redirects=True
            )
            response.raise_for_status()  # è§¦å‘4xx/5xxé”™è¯¯
            # åˆ†å—å†™å…¥ï¼ˆ1MB/å—ï¼‰ï¼Œé¿å…å†…å­˜æº¢å‡º
            with open(save_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=1024*1024):
                    if chunk:
                        f.write(chunk)
            if is_valid_pdf(save_path):
                print(f"âœ… ä¸‹è½½æˆåŠŸï¼š{os.path.basename(save_path)}")
                return True
            else:
                os.remove(save_path)
                print(f"âŒ æ— æ•ˆPDFæ–‡ä»¶ï¼Œé‡è¯•...")
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥ï¼ˆ{str(e)}ï¼‰ï¼Œé‡è¯•...")
            if os.path.exists(save_path):
                os.remove(save_path)
            time.sleep(2 * (retry + 1))  # é‡è¯•é—´éš”é€’å¢
    return False


# -------------------------- 1. arXivçˆ¬å–æ¨¡å—ï¼ˆé¢„å°æœ¬ï¼Œæ— æƒé™é™åˆ¶ï¼‰ --------------------------
def crawl_arxiv(max_results=None):
    print("\n===== å¼€å§‹çˆ¬å–arXiv =====")
    papers = []
    ns_uri = "http://www.w3.org/2005/Atom"  # XMLå‘½åç©ºé—´
    # ç²¾å‡†æœç´¢ï¼šæ ‡é¢˜/æ‘˜è¦åŒ…å«å…³é”®è¯ï¼Œä¸”å±äºè®¡ç®—æœºè§†è§‰æˆ–åŒ»å­¦ç‰©ç†é¢†åŸŸ
    search_query = f'"{SEARCH_PHRASE}" AND (cat:cs.CV OR cat:physics.med-ph)'
    params = {
        "search_query": search_query,
        "start": 0,
        "max_results": max_results if max_results is not None else MAX_RESULTS_PER_SITE,
        "sortBy": "submittedDate",  # æŒ‰æäº¤æ—¥æœŸæ’åºï¼ˆæœ€æ–°ä¼˜å…ˆï¼‰
        "sortOrder": "descending"
    }
    try:
        # è°ƒç”¨arXiv API
        response = requests.get(
            "http://export.arxiv.org/api/query",
            params=params,
            headers=get_random_headers(),
            timeout=TIMEOUT
        )
        root = ET.fromstring(response.text)
        entries = root.findall(f".//{{{ns_uri}}}entry")  # è§£æè®ºæ–‡åˆ—è¡¨
        print(f"arXivè§£æåˆ°{len(entries)}ç¯‡è®ºæ–‡")

        for entry in entries:
            # æå–æ ‡é¢˜ï¼ˆæ”¹è¿›ï¼šä½¿ç”¨æ›´é€šç”¨çš„æ–¹å¼æŸ¥æ‰¾æ ‡é¢˜å…ƒç´ ï¼Œå¤„ç†ä¸åŒçš„XMLç»“æ„ï¼‰
            title_elem = entry.find(f".//{{{ns_uri}}}title")
            title = ""
            if title_elem is not None:
                if title_elem.text:
                    title = title_elem.text.strip()
                elif title_elem.get_text():
                    title = title_elem.get_text().strip()
            title = title if title else "æ— æ ‡é¢˜"
            
            # æå–æ‘˜è¦å¹¶è½¬ä¸ºå°å†™ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
            summary_elem = entry.find(f".//{{{ns_uri}}}summary")
            summary = ""
            if summary_elem is not None:
                if summary_elem.text:
                    summary = summary_elem.text.lower()
                elif summary_elem.get_text():
                    summary = summary_elem.get_text().lower()
            summary = summary if summary else ""
            # è¿‡æ»¤æ— å…³è®ºæ–‡ï¼ˆæ ‡é¢˜æˆ–æ‘˜è¦å¿…é¡»åŒ…å«æ ¸å¿ƒå…³é”®è¯ï¼‰
            if SEARCH_PHRASE.lower() not in (title.lower() + summary):
                print(f"arXivè¿‡æ»¤æ— å…³ï¼š{title[:30]}...")
                continue
            # æå–PDFé“¾æ¥
            pdf_link = ""
            for link in entry.findall(f".//{{{ns_uri}}}link"):
                if link.get("type") == "application/pdf":
                    pdf_link = link.get("href")
                    break
            if not pdf_link:
                continue
            # æå–arXiv IDï¼ˆç”¨äºæ–‡ä»¶åï¼‰
            arxiv_id_match = re.search(r"arxiv.org/(?:abs|pdf)/(\d+\.\d+)", pdf_link)
            if arxiv_id_match:
                arxiv_id = arxiv_id_match.group(1)
            else:
                # å¤‡ç”¨åŒ¹é…æ¨¡å¼
                arxiv_id_match = re.search(r"arxiv.org/(?:abs|pdf)/([\w.-]+)", pdf_link)
                arxiv_id = arxiv_id_match.group(1).replace("/", "_") if arxiv_id_match else f"arxiv_{random.randint(1000000, 9999999)}"
            pdf_filename = f"arxiv_{arxiv_id}.pdf"
            pdf_path = os.path.join(PDF_SAVE_DIR, pdf_filename)
            # ä¸‹è½½PDF
            success = safe_download(pdf_link, pdf_path)
            # æå–ä½œè€…
            authors = [auth.find(f".//{{{ns_uri}}}name").text for auth in entry.findall(f".//{{{ns_uri}}}author") 
                      if auth.find(f".//{{{ns_uri}}}name") is not None and auth.find(f".//{{{ns_uri}}}name").text is not None]
            # ä¿å­˜è®ºæ–‡ä¿¡æ¯
            papers.append({
                "source": "arXiv",
                "id": arxiv_id,
                "title": title,
                "authors": ", ".join(authors) if authors else "æœªçŸ¥ä½œè€…",
                "pdf_link": pdf_link,
                "pdf_path": pdf_path,
                "status": "æˆåŠŸ" if success else "å¤±è´¥"
            })
            time.sleep(random.uniform(*REQUEST_DELAY))  # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
    except Exception as e:
        print(f"arXivçˆ¬å–å¤±è´¥ï¼š{e}")
    return papers


# -------------------------- è¾…åŠ©å‡½æ•°ï¼šè®¾ç½®å½“å‰ä¸»é¢˜ --------------------------
def set_search_phrase(phrase):
    """è®¾ç½®å½“å‰æœç´¢å…³é”®è¯ï¼Œå¹¶æ›´æ–°ç›¸å…³è·¯å¾„"""
    global SEARCH_PHRASE, PDF_SAVE_DIR, CSV_PATH
    SEARCH_PHRASE = phrase
    PDF_SAVE_DIR = f"{BASE_PDF_SAVE_DIR}/{SEARCH_PHRASE.replace(' ', '_')}"
    CSV_PATH = f"{BASE_CSV_PATH}_{SEARCH_PHRASE.replace(' ', '_')}.csv"
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(PDF_SAVE_DIR, exist_ok=True)
    print(f"\n===== å·²è®¾ç½®æœç´¢ä¸»é¢˜: {SEARCH_PHRASE} =====")

# -------------------------- ä¸»å‡½æ•°ï¼šæ•´åˆå¤šå¹³å°ç»“æœ --------------------------
def multi_source_crawl(max_results=None):
    """ä¸ºå½“å‰è®¾ç½®çš„ä¸»é¢˜æ‰§è¡Œçˆ¬å–ï¼Œåªçˆ¬å–arXivè®ºæ–‡
    
    Args:
        max_results: çˆ¬å–çš„è®ºæ–‡æ•°é‡ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼
    """
    all_papers = []
    # åªçˆ¬å–arXiv
    all_papers.extend(crawl_arxiv(max_results=max_results))

    # å»é‡ï¼ˆæŒ‰PDFé“¾æ¥ï¼Œé¿å…é‡å¤ä¸‹è½½ï¼‰
    unique_papers = []
    seen_links = set()
    for paper in all_papers:
        if paper["pdf_link"] not in seen_links:
            seen_links.add(paper["pdf_link"])
            unique_papers.append(paper)
    print(f"\n===== å»é‡åå…±{len(unique_papers)}ç¯‡è®ºæ–‡ =====")

    # ä¿å­˜ç»“æœåˆ°CSV
    with open(CSV_PATH, "w", newline="", encoding="utf-8") as f:
        if unique_papers:
            fieldnames = unique_papers[0].keys()
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(unique_papers)
    print(f"ç»“æœå·²ä¿å­˜è‡³ï¼š{CSV_PATH}")
    print(f"PDFæ–‡ä»¶ä¿å­˜ç›®å½•ï¼š{os.path.abspath(PDF_SAVE_DIR)}")
    return unique_papers

# -------------------------- æ‰¹é‡çˆ¬å–å‡½æ•°ï¼šæ”¯æŒå¤šä¸»é¢˜ --------------------------
def batch_crawl(topics):
    """æ‰¹é‡çˆ¬å–å¤šä¸ªä¸»é¢˜
    
    Args:
        topics: ä¸»é¢˜åˆ—è¡¨ï¼Œå¦‚ ["medical image registration", "computer vision"]
        
    Returns:
        dict: æ¯ä¸ªä¸»é¢˜å¯¹åº”çš„çˆ¬å–ç»“æœ
    """
    if not topics:
        print("è­¦å‘Šï¼šæœªæä¾›ä¸»é¢˜åˆ—è¡¨ï¼Œä½¿ç”¨é»˜è®¤ä¸»é¢˜")
        topics = [DEFAULT_SEARCH_PHRASE]
    
    results = {}
    for topic in topics:
        print(f"\n{'='*50}")
        print(f"å¼€å§‹çˆ¬å–ä¸»é¢˜: {topic}")
        print(f"{'='*50}")
        # è®¾ç½®å½“å‰ä¸»é¢˜
        set_search_phrase(topic)
        # æ‰§è¡Œçˆ¬å–
        papers = multi_source_crawl()
        results[topic] = papers
    
    print(f"\n{'='*50}")
    print("æ‰¹é‡çˆ¬å–å®Œæˆï¼")
    print(f"{'='*50}")
    for topic, papers in results.items():
        print(f"{topic}: {len(papers)}ç¯‡è®ºæ–‡")
    
    return results


if __name__ == "__main__":
    # è‡ªåŠ¨å®‰è£…ä¾èµ–
    try:
        from fake_useragent import UserAgent
    except ImportError:
        print("å®‰è£…fake_useragent...")
        os.system("pip install fake_useragent")
        from fake_useragent import UserAgent
    
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        print("å®‰è£…beautifulsoup4...")
        os.system("pip install beautifulsoup4")
        from bs4 import BeautifulSoup
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(BASE_PDF_SAVE_DIR, exist_ok=True)
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='arXivè®ºæ–‡çˆ¬å–å·¥å…·')
    parser.add_argument('-n', '--num', type=int, default=None, help='çˆ¬å–çš„è®ºæ–‡æ•°é‡')
    args = parser.parse_args()
    
    # ä½¿ç”¨æŒ‡å®šæ•°é‡çˆ¬å–arXivè®ºæ–‡
    multi_source_crawl(max_results=args.num)